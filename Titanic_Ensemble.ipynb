{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Titanic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.read_csv('titanicTrain.csv') # Reads in the test data\n",
    "trainSurvived = data['Survived'].values # Separates out the survival data from the input data\n",
    "trainData = data.drop('Survived', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.isnull().sum() # This shows us that null values are present in Age, Cabin, and Embarked.  These will need to be handled\n",
    "                        # before proceeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transformer will delete the columns 'Name', 'Ticket', and 'Cabin'\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "class DeleteColumns(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X = X.drop('Name', 1) # Removes the Name Column because we already have an ID column\n",
    "        X = X.drop('Ticket', 1) # Remove the Ticket column because it is not pertinent\n",
    "        X = X.drop('Cabin', 1) # Removes the Cabin column because it is so sparse\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This transformer will remove any rows with missing categorical attributes ('Embarked' and 'Sex')\n",
    "class DeleteRows(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        X = X[pd.notnull(X['Embarked'])] # Removes the rows missing values in Embarked\n",
    "        X = X[pd.notnull(X['Sex'])] # Removes the rows missing values in Sex\n",
    "        return X # Returns the updated \n",
    "    \n",
    "rowDelete = DeleteRows()\n",
    "trainRowDelete = rowDelete.transform(trainData)\n",
    "trainRowDelete.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This transformer will select the appropriate attributes from the dataframe for each pipeline\n",
    "class DataFrameSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, attribute_names):\n",
    "        self.attribute_names = attribute_names\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.attribute_names].values # Returns the columns with the inputted titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will create the Pipeline for the cleaning of the data\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Imputer\n",
    "import category_encoders as ce\n",
    "\n",
    "trainData_Num = ['Pclass', 'Age', 'SibSp', 'Parch'] # This is the list of columns with numerical attributes to be\n",
    "                                                                    # used in the pipeline\n",
    "trainData_Cat = ['Sex', 'Embarked'] # A list of the columns with categorical attributes to be used in the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PreCleaning Pipeline\n",
    "    # This pipeline will remove columns that we do not want as well as any rows containing null values\n",
    "pre_pipeline = Pipeline([\n",
    "    ('colDelete', DeleteColumns()), # Removes the Name, Ticket, and Cabin columns\n",
    "    ('rowDelete', DeleteRows()) # Removes any rows containing null values within the columns Embarked and Sex\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Numerical Pipeline\n",
    "num_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(trainData_Num)), # Selects the numerical data\n",
    "    ('imputer', Imputer(strategy = 'median')), # Will replace missing values with the median of the column\n",
    "    ('std_scaler', StandardScaler()) # Will scale the values\n",
    "])\n",
    "numTrain = num_pipeline.fit_transform(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The Categorical Pipeline\n",
    "cat_pipeline = Pipeline([\n",
    "    ('selector', DataFrameSelector(trainData_Cat)),  # Selects the categorical data\n",
    "    ('1hot', ce.OneHotEncoder()) # Will transform categorical data into numerical\n",
    "])\n",
    "catTrain = cat_pipeline.fit_transform(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "full_pipeline = FeatureUnion(transformer_list = [\n",
    "    ('num_pipeline', num_pipeline),\n",
    "    ('cat_pipeline', cat_pipeline)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanicTrain.csv') # Reads in the test data\n",
    "titanicClean = pre_pipeline.fit_transform(titanic) # Removes rows with null values in 'Sex' and 'Embarked'\n",
    "\n",
    "y = titanicClean['Survived'] # Separates out the survival data from the input data\n",
    "X = titanicClean.drop('Survived', 1)\n",
    "\n",
    "X_prepared = full_pipeline.fit_transform(X) # Sends the pre-cleaned data through the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_prepared, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we must choose a model to predict the likelihood of survival.  Because we are choosing only between whether or not some\n",
    "# survived, we will use a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Classifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_clf = LogisticRegression() # Initializes the classifiers\n",
    "\n",
    "param_grid = [\n",
    "    {'penalty': ['l1', 'l2'], 'C':[0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "]\n",
    "grid_search = GridSearchCV(log_clf, param_grid, cv = 5, scoring = 'accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "log_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "param_grid = [\n",
    "    {'n_estimators':[500, 1000], 'max_features': [3, 5, 7, 9], 'n_jobs': [-1]}\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, cv = 5, scoring = 'accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "rf_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = SVC()\n",
    "\n",
    "param_grid = [\n",
    "    {'probability':[True], 'C': [0.01, 0.1, 1.0, 10.0, 100.0], 'kernel': ['rbf', 'poly', 'sigmoid'], \n",
    "     'gamma': [0.01, 0.1, 1.0, 10.0, 100.0, 'auto']}\n",
    "]\n",
    "grid_search = GridSearchCV(svm_clf, param_grid, cv = 5, scoring = 'accuracy')\n",
    "y_train\n",
    "grid_search.fit(X_train, y_train)\n",
    "svm_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier()\n",
    "\n",
    "param_grid = [\n",
    "    {'loss': ['log'], 'penalty': ['none', 'l1', 'l2', 'elasticnet'], 'alpha': [0.00001, 0.000001],\n",
    "    'n_jobs': [-1]}\n",
    "]\n",
    "grid_search = GridSearchCV(sgd_clf, param_grid, cv = 5, scoring = 'accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "sgd_clf = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf), ('sgd', sgd_clf)],\n",
    "    voting = 'soft') # bases the prediction on the highest class probability\n",
    "\n",
    "for clf in (log_clf, rf_clf, svm_clf, sgd_clf, voting_clf): # Will iterate through each classifier, including the voting classifier\n",
    "    clf.fit(X_train, y_train) # Fits each classifier to the data\n",
    "    y_pred = clf.predict(X_test) # Records the prediction of the classifier on the test set\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred)) # Prints the accuracy score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
